Status;Title;Abstract;Authors;Year
ACCEPTED;A cloud-based framework for machine learning workloads and applications;In this paper we propose a distributed architecture to provide machine learning practitioners with a set of tools and cloud services that cover the whole machine learning development cycle: ranging from the models creation, training, validation and testing to the models serving as a service, sharing and publication. In such respect, the DEEP-Hybrid-DataCloud framework allows transparent access to existing e-Infrastructures, effectively exploiting distributed resources for the most compute-intensive tasks coming from the machine learning development cycle. Moreover, it provides scientists with a set of Cloud-oriented services to make their models publicly available, by adopting a serverless architecture and a DevOps approach, allowing an easy share, publish and deploy of the developed models. © 2013 IEEE.;Lopez Garcia A., Tran V., Alic A.S., Caballer M., Plasencia I.C., Costantini A., Dlugolinsky S., Duma D.C., Donvito G., Gomes J., Heredia Cacha I., De Lucas J.M., Ito K., Kozlov V.Y., Nguyen G., Orviz Fernandez P., Sustr Z., Wolniewicz P., Antonacci M., Zu Castell W., David M., Hardt M., Lloret Iglesias L., Molto G., Plociennik M.;2020
REJECTED;A Machine Learning-Based Approach for Efficient Cloud Service Selection;Cloud computing can be considered a revolutionizing invention of decay. The computing resources are externally supplied to the user with benefits of scalability, accessibility, pay-as-you-go, serverless, etc. With its ever-growing market, there is a multitude of cloud service providers (CSPs) with different offerings available to small and medium enterprises (SMEs) or users. Due to the availability of numerous CSPs with different offerings, it becomes complicated for the user to pick the right services. In the presented paper, a supervised learning-based model based on Random Forest Regressor is proposed. The proposed model has been trained for Multi Criteria Decision Making Methods (MCDM) methods such as Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), VlseKriterijumska Optimizacija I Kompromisno Resenje (VIKOR), and Weighted Sum Method (WSM). The score obtained from these MCDM methods has been used to rank the services. Results show the acceptability of the proposed model in the cloud environment. © 2022, Springer Nature Switzerland AG.;Gandhi U., Bothera A., Garg N., Neeraj, Gupta I.;2022
REJECTED;A migration-based approach to execute long-duration multi-cloud serverless functions;Serverless Computing is emerging as an undeniable paradigm for the deployment of (multi)cloud applications. It is mainly characterized by the use of stateless loosely-coupled functions that are composed together to perform useful actions. This approach, contrarily to monolithic one, makes easier the maintenance and the evolution of the applications, since the functions can be independently revised and reprogrammed. However, one principle in Serverless computing is that function execution should be within a short duration (five minutes max in most Cloud provider platforms), after which the function is abruptly terminated even if it has not completed its task. Moreover, the max duration cannot be extended without a negative effect on the platform performance. This leads to prevent functions requiring longer time from being adopted as Serverless functions. This paper deals with this drawback. It proposes a distributed migration-based approach which promotes the execution of long-duration Serverless functions: each running function that reaches the maximum duration limit is repeatedly transferred to another cloud platform where it is carried on. In this aim, the migration-based system architecture, the migration technique and the migration algorithm are described. The proposed approach use is illustrated by a case study: a generic machine learning application built over the scientific platform ANTDROID. Copyright © by the paper’s authors.;Soltani B., Ghenai A., Zeghib N.;2018
ACCEPTED;A serverless gateway for event-driven machine learning inference in multiple clouds;Serverless computing and, in particular, the functions as a service model has become a convincing paradigm for the development and implementation of highly scalable applications in the cloud. This is due to the transparent management of three key functionalities: triggering of functions due to events, automatic provisioning and scalability of resources, and fine-grained pay-per-use. This article presents a serverless web-based scientific gateway to execute the inference phase of previously trained machine learning and artificial intelligence models. The execution of the models is performed both in Amazon Web Services and in on-premises clouds with the OSCAR framework for serverless scientific computing. In both cases, the computing infrastructure grows elastically according to the demand adopting scale-to-zero approaches to minimize costs. The web interface provides an improved user experience by simplifying the use of the models. The usage of machine learning in a computing platform that can use both on-premises clouds and public clouds constitutes a step forward in the adoption of serverless computing for scientific applications. © 2021 John Wiley & Sons, Ltd.;Naranjo D.M., Risco S., Moltó G., Blanquer I.;2021
REJECTED;Accelerated serverless computing based on GPU virtualization;This paper introduces a platform to support serverless computing for scalable event-driven data processing that features a multi-level elasticity approach combined with virtualization of GPUs. The platform supports the execution of applications based on Docker containers in response to file uploads to a data storage in order to perform the data processing in parallel. This is managed by an elastic Kubernetes cluster whose size automatically grows and shrinks depending on the number of files to be processed. To accelerate the processing time of each file, several approaches involving virtualized access to GPUs, either locally or remote, have been evaluated. A use case that involves the inference based on deep learning techniques on transthoracic echocardiography imaging has been carried out to assess the benefits and limitations of the platform. The results indicate that the combination of serverless computing and GPU virtualization introduce an efficient and cost-effective event-driven accelerated computing approach that can be applied for a wide variety of scientific applications. © 2020 Elsevier Inc.;Naranjo D.M., Risco S., de Alfonso C., Pérez A., Blanquer I., Moltó G.;2020
REJECTED;Adaptive and Efficient Streaming Time Series Forecasting with Lambda Architecture and Spark;The rise of the Internet of Things (IoT) devices and the streaming platform has tremendously increased the data in motion or streaming data. It incorporates a wide variety of data, for example, social media posts, online gamers in-game activities, mobile or web application logs, online e-commerce transactions, financial trading, or geospatial services. Accurate and efficient forecasting based on real-time data is a critical part of the operation in areas like energy utility consumption, healthcare, industrial production, supply chain, weather forecasting, financial trading, agriculture, etc. Statistical time series forecasting methods like Autoregression (AR), Autoregressive integrated moving average (ARIMA), and Vector Autoregression (VAR), face the challenge of concept drift in the streaming data, i.e., the properties of the stream may change over time. Another challenge is the efficiency of the system to update the Machine Learning (ML) models which are based on these algorithms to tackle the concept drift. In this paper, we propose a novel framework to tackle both of these challenges. The challenge of adaptability is addressed by applying the Lambda architecture to forecast future state based on three approaches simultaneously: batch (historic) data-based prediction, streaming (real-time) data-based prediction, and hybrid prediction by combining the first two. To address the challenge of efficiency, we implement a distributed VAR algorithm on top of the Apache Spark big data platform. To evaluate our framework, we conducted experiments on streaming time series forecasting with four types of data sets of experiments: data without drift (no drift), data with gradual drift, data with abrupt drift and data with mixed drift. The experiments show the differences of our three forecasting approaches in terms of accuracy and adaptability. © 2020 IEEE.;Pandya A., Odunsi O., Liu C., Cuzzocrea A., Wang J.;2020
REJECTED;AI on the Move: From On-Device to On-Multi-Device;On-Device AI is an emerging paradigm that aims to make devices more intelligent, autonomous and proactive by equipping them with machine and deep learning routines for robust decision making and optimal execution in devices' operations. On-Device intelligence promises the possibility of computing huge amounts of data close to its source, e.g., sensor and multimedia data. By doing so, devices can complement their counterpart cloud services with more sophisticated functionality to provide better applications and services. However, increased computational capabilities of smart devices, wearables and IoT devices along with the emergence of services at the Edge of the network are driving the trend of migrating and distributing computation between devices. Indeed, devices can reduce the burden of executing resource intensive tasks via collaborations in the wild. While several work has shown the benefits of an opportunistic collaboration of a device with others, not much is known regarding how devices can be organized as a group as they move together. In this paper, we contribute by analyzing how dynamic group organization of devices can be utilized to distribute intelligence on the moving Edge. The key insight is that instead of On-Device solutions complementing with cloud, dynamic groups can be formed to complement each other in an On-Multi-Device manner. Thus, we highlight the challenges and opportunities from extending the scope of On-Device AI from an egocentric view to a collaborative, multi-device view. © 2019 IEEE.;Flores H., Nurmi P., Hui P.;2019
ACCEPTED;AMPS-Inf: Automatic Model Partitioning for Serverless Inference with Cost Efficiency;The salient pay-per-use nature of serverless computing has driven its continuous penetration as an alternative computing paradigm for various workloads. Yet, challenges arise and remain open when shifting machine learning workloads to the serverless environment. Specifically, the restriction on the deployment size over serverless platforms combining with the complexity of neural network models makes it difficult to deploy large models in a single serverless function. In this paper, we aim to fully exploit the advantages of the serverless computing paradigm for machine learning workloads targeting at mitigating management and overall cost while meeting the response-time Service Level Objective (SLO). We design and implement AMPS-Inf, an autonomous framework customized for model inferencing in serverless computing. Driven by the cost-efficiency and timely-response, our proposed AMPS-Inf automatically generates the optimal execution and resource provisioning plans for inference workloads. The core of AMPS-Inf relies on the formulation and solution of a Mixed-Integer Quadratic Programming problem for model partitioning and resource provisioning with the objective of minimizing cost without violating response time SLO. We deploy AMPS-Inf on the AWS Lambda platform, evaluate with the state-of-the-art pre-trained models in Keras including ResNet50, Inception-V3 and Xception, and compare with Amazon SageMaker and three baselines. Experimental results demonstrate that AMPS-Inf achieves up to 98% cost saving without degrading response time performance. © 2021 ACM.;Jarachanthan J., Chen L., Xu F., Li B.;2021
REJECTED;An empirical study on challenges of application development in serverless computing;Serverless computing is an emerging paradigm for cloud computing, gaining traction in a wide range of applications such as video processing and machine learning. This new paradigm allows developers to focus on the development of the logic of serverless computing based applications (abbreviated as serverless-based applications) in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, it also introduces new challenges on the design, implementation, and deployment of serverless-based applications, and current serverless computing platforms are far away from satisfactory. However, to the best of our knowledge, these challenges have not been well studied. To fill this knowledge gap, this paper presents the first comprehensive study on understanding the challenges in developing serverless-based applications from the developers' perspective. We mine and analyze 22,731 relevant questions from Stack Overflow (a popular Q&A website for developers), and show the increasing popularity trend and the high difficulty level of serverless computing for developers. Through manual inspection of 619 sampled questions, we construct a taxonomy of challenges that developers encounter, and report a series of findings and actionable implications. Stakeholders including application developers, researchers, and cloud providers can leverage these findings and implications to better understand and further explore the serverless computing paradigm. © 2021 ACM.;Wen J., Chen Z., Liu Y., Lou Y., Ma Y., Huang G., Jin X., Liu X.;2021
ACCEPTED;Automatic Hyperparameter Optimization for Arbitrary Neural Networks in Serverless AWS Cloud;Deep Neural Networks are the most efficient method to solve many challenging problems. The importance of the subject can be demonstrated by the fact that the 2019 Turing Award was given to the godfathers of AI (and Neural Networks) Yoshua Bengio, Geoffrey Hinton, and Yann LeCun. In spite of the numerous advancements in the field, most of the models are being tuned manually. Accurate models became especially important during the novel coronavirus pandemic.Many day-to-day decisions depend on the model predictions affecting billions of people. We implemented a flexible automatic real-time hyperparameter tuning approach for arbitrary DNN models written in Python and Keras without manual steps. All of the existing tuning libraries require manual steps (like hyperopt, Scikit-Optimize or SageMaker). We provide an innovative methodology to automate hyper-parameter tuning for an arbitrary Neural Network model source code, utilizing Serverless Cloud and implementing revolutionary microservices, security, interoperability and orchestration. Our methodology can be used in numerous applications, including Information and Communication Systems. © 2021 IEEE.;Kaplunovich A., Yesha Y.;2021
ACCEPTED;Automatic Tuning of Hyperparameters for Neural Networks in Serverless Cloud;Deep Neural Networks are used to solve the most challenging world problems. In spite of the numerous advancements in the field, most of the models are being tuned manually. Experienced Data Scientists have to manually optimize hyperparameters, such as dropout rate, learning rate or number of neurons for Big Data applications. We have implemented a flexible automatic real-time hyperparameter tuning methodology. It works for arbitrary models written in Python and Keras. We also utilized state of the art Cloud services such as trigger based serverless computing (Lambda), and advanced GPU instances to implement automation, reliability and scalability.The existing tuning libraries, such as hyperopt, Scikit-Optimize or SageMaker, require developers to provide a list of hyperparameters and the range of their values manually. Our novel approach detects potential hyperparameters automatically from the source code, updates the original model to tune the parameters, runs the evaluation in the Cloud on spot instances, finds the optimal hyperparameters, and saves the results in the No-SQL database. The methodology can be applied to numerous Big Data Machine Learning systems. © 2020 IEEE.;Kaplunovich A., Yesha Y.;2020
ACCEPTED;BARISTA: Efficient and scalable serverless serving system for deep learning prediction services;Pre-trained deep learning models are increasingly being used to offer a variety of compute-intensive predictive analytics services such as fitness tracking, speech, and image recognition. The stateless and highly parallelizable nature of deep learning models makes them well-suited for serverless computing paradigm. However, making effective resource management decisions for these services is a hard problem due to the dynamic workloads and diverse set of available resource configurations that have different deployment and management costs. To address these challenges, we present a distributed and scalable deep-learning prediction serving system called Barista and make the following contributions. First, we present a fast and effective methodology for forecasting workloads by identifying various trends. Second, we formulate an optimization problem to minimize the total cost incurred while ensuring bounded prediction latency with reasonable accuracy. Third, we propose an efficient heuristic to identify suitable compute resource configurations. Fourth, we propose an intelligent agent to allocate and manage the compute resources by horizontal and vertical scaling to maintain the required prediction latency. Finally, using representative real-world workloads for an urban transportation service, we demonstrate and validate the capabilities of Barista. © 2019 IEEE.;Bhattacharjee A., Chhokra A.D., Kang Z., Sun H., Gokhale A., Karsai G.;2019
ACCEPTED;Batch: Machine learning inference serving on serverless platforms with adaptive batching;Serverless computing is a new pay-per-use cloud service paradigm that automates resource scaling for stateless functions and can potentially facilitate bursty machine learning serving. Batching is critical for latency performance and cost-effectiveness of machine learning inference, but unfortunately it is not supported by existing serverless platforms due to their stateless design. Our experiments show that without batching, machine learning serving cannot reap the benefits of serverless computing. In this paper, we present BATCH, a framework for supporting efficient machine learning serving on serverless platforms. BATCH uses an optimizer to provide inference tail latency guarantees and cost optimization and to enable adaptive batching support. We prototype BATCH atop of AWS Lambda and popular machine learning inference systems. The evaluation verifies the accuracy of the analytic optimizer and demonstrates performance and cost advantages over the state-of-the-art method MArk and the state-of-the-practice tool SageMaker. © 2020 IEEE.;Ali A., Pinciroli R., Yan F., Smirni E.;2020
ACCEPTED;Behavior analysis using serverless machine learning;This paper supplies a route for using the Watson Machine Learning API on IBM Cloud to carry out serverless data analytics using machine learning as a service. Transforming the large amount of data produced by an organization into intelligence can be done using advanced analytics methods such as using a modified Mahalanobis Distance algorithm for synthesis of correlation data under the purview of machine learning. Further refinement of correlation data is done using a Multivariate Reliability Classifier model. The consumption of this advanced analytics service can be done in a serverless manner where the developer only must be concerned with how the data is analyzed, i.e., scoring, batch or stream models with a continuous learning system without the outlay of hardware upon which to train those models. This paper examines the usage of such serverless AI systems in the scope of user behavior analysis over varied demographics. © 2019 Bharati Vidyapeeth, New Delhi. Copy Right in Bulk will be transferred to IEEE by Bharati Vidyapeeth.;Damkevala D., Lunavara R., Kosamkar M., Jayachandran S.;2019
ACCEPTED;Cirrus: A Serverless Framework for End-To-end ML Workflows;Machine learning (ML) workflows are extremely complex. The typical workflow consists of distinct stages of user interaction, such as preprocessing, training, and tuning, that are repeatedly executed by users but have heterogeneous computational requirements. This complexity makes it challenging for ML users to correctly provision and manage resources and, in practice, constitutes a significant burden that frequently causes over-provisioning and impairs user productivity. Serverless computing is a compelling model to address the resource management problem, in general, but there are numerous challenges to adopt it for existing ML frameworks due to significant restrictions on local resources. This work proposes Cirrus-An ML framework that automates the end-To-end management of datacenter resources for ML workflows by efficiently taking advantage of serverless infrastructures. Cirrus combines the simplicity of the serverless interface and the scalability of the serverless infrastructure (AWS Lambdas and S3) to minimize user effort. We show a design specialized for both serverless computation and iterative ML training is needed for robust and efficient ML training on serverless infrastructure. Our evaluation shows that Cirrus outperforms frameworks specialized along a single dimension: Cirrus is 100x faster than a general purpose serverless system [36] and 3.75x faster than specialized ML frameworks for traditional infrastructures [49]. © 2019 ACM.;Carreira J., Fonseca P., Tumanov A., Zhang A., Katz R.;2019
REJECTED;Contention-aware container placement strategy for docker swarm with machine learning based clustering algorithms;Containerization technology utilizes operating system level virtualization to package applications to run with required libraries and are isolated from other processes on the same host. Lightweight and quick deployment make containers popular in many data centers. Running distributed applications in data centers usually involves multiple clusters of machines. Docker Swarm is a container orchestration tool for managing a cluster of Docker containers and their hosts. However, Docker Swarm’s scheduler does not consider resource utilization when placing containers in a cluster. This paper first investigated performance interference in container clusters. Our experimental study showed that distributed applications’ performance can be degraded when co-located with other containers which aggressively consume resources. A new scheduler is proposed to improve performance while keeping high resource utilization. The experimental results demonstrated that the proposed prototype with machine learning based clustering algorithms could effectively improve distributed applications’ performance by up to 14.5% with an average at around 12%. This work also provides theoretical bounds for the container placement problem. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.;Chiang R.C.;2020
REJECTED;Context agnostic trajectory prediction based on _-architecture;Predicting the next position of movable objects has been a problem for at least the last three decades, referred to as ‘trajectory prediction’. In our days, the vast amounts of data being continuously produced add the big data dimension to the trajectory prediction problem, which we are trying to tackle by creating a _-Architecture based analytics platform. This platform performs both batch and stream analytics tasks and then combines them to perform analytical tasks that cannot be performed by analyzing any of these layers by itself. The biggest benefit of this platform is its context agnostic trait, which allows us to use it for any use case, as long as a time-stamped geo-location stream is provided. The experimental results presented prove that each part of the _-Architecture performs well at certain targets, making a combination of these parts a necessity in order to improve the overall accuracy and performance of the platform. © 2019 Elsevier B.V.;Psomakelis E., Tserpes K., Zissis D., Anagnostopoulos D., Varvarigou T.;2020
ACCEPTED;Cross-Platform Performance Evaluation of Stateful Serverless Workflows;Serverless computing, with its inherent event-driven design along with instantaneous scalability due to cloud-provider managed infrastructure, is starting to become a de-facto model for deploying latency critical user-interactive services. However, as much as they are suitable for event-driven services, their stateless nature is a major impediment for deploying long-running stateful applications. While commercial cloud providers offer a variety of solutions that club serverless functions along with intermediate storage to maintain application state, they are still far from optimized for deploying stateful applications at scale. More specifically, factors such as storage latency and scalability, network bandwidth, and deployment costs play a crucial role in determining whether current serverless applications are suitable for stateful workloads. In this paper, we evaluate the two widely-used stateful server-less offerings, Azure Durable functions and AWS Step functions, to quantify their effectiveness for implementing complex stateful workflows. We conduct a detailed measurement-driven characterization study with two real-world use cases, machine learning pipelines (inference and training) and video analytics, in order to characterize the different performance latency and cost tradeoffs. We observe from our experiments that AWS is suitable for workloads with higher degree of parallelism, while Azure durable entities offer a simplified framework that enables quicker application development. Overall, AWS is 89% more expensive than Azure for machine learning training application while Azure is 2_ faster than AWS for the machine learning inference application. Our results indicate that Azure durable is extremely inefficient in implementing parallel processing. Furthermore, we summarize the key findings from our characterization, which we believe to be insightful for any cloud tenant who has the problem of choosing an appropriate cloud vendor and offering, when deploving stateful workloads on serverless platforms, © 2021 IEEE.;Shahidi N., Gunasekaran J.R., Kandemir M.T.;2021
REJECTED;Deep learning architectures in emerging cloud computing architectures: Recent development, challenges and next research trend;The challenges of the conventional cloud computing paradigms motivated the emergence of the next generation cloud computing architectures. The emerging cloud computing architectures generate voluminous amount of data that are beyond the capability of the shallow intelligent algorithms to process. Deep learning algorithms, with their ability to process large-scale datasets, have recently started gaining tremendous attentions from researchers to solve problem in the emerging cloud computing architectures. However, no comprehensive literature review exists on the applications of deep learning architectures to solve complex problems in emerging cloud computing architectures. To fill this gap, we conducted a comprehensive literature survey on the applications of deep learning architectures in emerging cloud computing architectures. The survey shows that the adoption of deep learning architectures in emerging cloud computing architectures are increasingly becoming an interesting research area. We introduce a new taxonomy of deep learning architectures for emerging cloud computing architectures and provide deep insights into the current state-of-the-art active research works on deep learning to solve complex problems in emerging cloud computing architectures. The synthesis and analysis of the articles as well as their limitation are presented. A lot of challenges were identified in the literature and new future research directions to solve the identified challenges are presented. We believed that this article can serve as a reference guide to new researchers and an update for expert researchers to explore and develop more deep learning applications in the emerging cloud computing architectures. © 2020 Elsevier B.V.;Jauro F., Chiroma H., Gital A.Y., Almutairi M., Abdulhamid S.M., Abawajy J.H.;2020
REJECTED;DevOps and Quality Management in Serverless Computing: The RADON Approach;The onset of microservices and serverless computer solutions has forced an ever-increasing demand for tools and techniques to establish and maintain the quality of infrastructure code, the blueprint that drives the operationalization of large-scale software systems. In the EU H2020 project RADON, we propose a machine-learning approach to elaborate and evolve Infrastructure-as-Code as part of a full-fledged industrial-strength DevOps pipeline. This paper illustrates RADON and shows our research roadmap. © 2021, Springer Nature Switzerland AG.;Dalla Palma S., Garriga M., Di Nucci D., Tamburri D.A., Van Den Heuvel W.-J.;2021
ACCEPTED;Distributed double machine learning with a serverless architecture;This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation DoubleML-Serverless for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs. © 2021 Copyright held by the owner/author(s).;Kurz M.S.;2021
ACCEPTED;Distributed Machine Learning with a Serverless Architecture;The need to scale up machine learning, in the presence of a rapid growth of data both in volume and in variety, has sparked broad interests to develop distributed machine learning systems, typically based on parameter servers. However, since these systems are based on a dedicated cluster of physical or virtual machines, they have posed non-trivial cluster management overhead to machine learning practitioners and data scientists. In addition, there exists an inherent mismatch between the dynamically varying resource demands during a model training job and the inflexible resource provisioning model of current cluster-based systems.In this paper, we propose SIREN, an asynchronous distributed machine learning framework based on the emerging serverless architecture, with which stateless functions can be executed in the cloud without the complexity of building and maintaining virtual machine infrastructures. With SIREN, we are able to achieve a higher level of parallelism and elasticity by using a swarm of stateless functions, each working on a different batch of data, while greatly reducing system configuration overhead. Furthermore, we propose a scheduler based on Deep Reinforcement Learning to dynamically control the number and memory size of the stateless functions that should be used in each training epoch. The scheduler learns from the training process itself, in pursuit for the minimum possible training time given a cost. With our real-world prototype implementation on AWS Lambda, extensive experimental results have shown that SIREN can reduce model training time by up to 44%, as compared to traditional machine learning training benchmarks on AWS EC2 at the same cost. © 2019 IEEE.;Wang H., Niu D., Li B.;2019
ACCEPTED;Dorylus: Affordable, scalable, and accurate GNN training with distributed CPU servers and serverless threads;A graph neural network (GNN) enables deep learning on structured graph data. There are two major GNN training obstacles: 1) it relies on high-end servers with many GPUs which are expensive to purchase and maintain, and 2) limited memory on GPUs cannot scale to today’s billion-edge graphs. This paper presents Dorylus: a distributed system for training GNNs. Uniquely, Dorylus can take advantage of serverless computing to increase scalability at a low cost. The key insight guiding our design is computation separation. Computation separation makes it possible to construct a deep, bounded-asynchronous pipeline where graph and tensor parallel tasks can fully overlap, effectively hiding the network latency incurred by Lambdas. With the help of thousands of Lambda threads, Dorylus scales GNN training to billion-edge graphs. Currently, for large graphs, CPU servers offer the best performance per dollar over GPU servers. Just using Lambdas on top of Dorylus offers up to 2.75_ more performance-per-dollar than CPU-only servers. Concretely, Dorylus is 1.22_ faster and 4.83_ cheaper than GPU servers for massive sparse graphs. Dorylus is up to 3.8_ faster and 10.7_ cheaper compared to existing sampling-based systems. © 2021 by The USENIX Association. All rights reserved.;Thorpe J., Qiao Y., Eyolfson J., Teng S., Hu G., Jia Z., Wei J., Vora K., Netravali R., Kim M., Xu G.H.;2021
ACCEPTED;Edge-adaptable serverless acceleration for machine learning Internet of Things applications;Serverless computing is an emerging event-driven programming model that accelerates the development and deployment of scalable web services on cloud computing systems. Though widely integrated with the public cloud, serverless computing use is nascent for edge-based, Internet of Things (IoT) deployments. In this work, we present STOIC (serverless teleoperable hybrid cloud), an IoT application deployment and offloading system that extends the serverless model in three ways. First, STOIC adopts a dynamic feedback control mechanism to precisely predict latency and dispatch workloads uniformly across edge and cloud systems using a distributed serverless framework. Second, STOIC leverages hardware acceleration (e.g., GPU resources) for serverless function execution when available from the underlying cloud system. Third, STOIC can be configured in multiple ways to overcome deployment variability associated with public cloud use. We overview the design and implementation of STOIC and empirically evaluate it using real-world machine learning applications and multitier IoT deployments (edge and cloud). Specifically, we show that STOIC can be used for training image processing workloads (for object recognition)—once thought too resource-intensive for edge deployments. We find that STOIC reduces overall execution time (response latency) and achieves placement accuracy that ranges from 92% to 97%. © 2020 John Wiley & Sons, Ltd.;Zhang M., Krintz C., Wolski R.;2021
ACCEPTED;Enabling Cost-Effective, SLO-Aware Machine Learning Inference Serving on Public Cloud;The remarkable advances of Machine Learning (ML) have spurred an increasing demand for ML-as-a-Service on publiccloud: developers train and publish ML models as online services to provide low-latency inference for dynamic queries. The primarychallenge of ML model serving is to meet the response-time Service-Level Objectives (SLOs) of inference workloads while minimizingserving cost. In this paper, we proposes MArk (Model Ark), a general-purpose inference serving system, to tackle the dual challenge ofSLO compliance and cost effectiveness. MArk employs three design choices tailored to inference workload. First, MArk dynamicallybatches requests and opportunistically serves them using expensive hardware accelerators (e.g., GPU) for improved performance-costratio. Second, instead of relying on feedback control scaling or over-provisioning to serve dynamic workload, which can be too slow ortoo expensive, MArk employs predictive autoscaling to hide the provisioning latency at low cost. Third, given the stateless nature ofinference serving, MArk exploits the flexible, yet costly serverless instances to cover occasional load spikes that are hard to predict. Weevaluated the performance of MArk using several state-of-the-art ML models trained in TensorFlow, MXNet, and Keras. Compared withthe premier industrial ML serving platform SageMaker, MArk reduces the serving cost up to 7.8while achieving even better latencyperformance. IEEE;Zhang C., Yu M., wang W., Yan F.;2020
ACCEPTED;Enhancing automated FaaS with cost-aware provisioning of cloud resources;Compute resources are becoming more diverse, more specialized, and more physically distributed every day. To properly use these resources, the new paradigm of serverless computing aims to abstract away the complexity associated with resource configuration and workload deployment. Building on this serverless architecture are efforts for automated resource selection and automated task distribution and coordination. As computing resources advance, so too does application-specific optimizations, as is the case of deep learning on GPUs or even the more specialized TPUs. To better navigate the efficient and effective use of these diverse resources, we present the addition of automated, cost-aware provisioning of cloud resources to a state-of-the-art automated serverless framework. By automating the selection, provisioning, and configuration of cloud resources, our framework, which we call DELTA+, will enable truly cost-aware usage of the cloud by navigating complex computational tradeoffs. In this proposal, we will outline how we plan to introduce burstable cloud computing to automated serverless infrastructure and to present our initial findings with respect to system design and performance. © 2021 IEEE.;Baughman M., Foster I., Chard K.;2021
ACCEPTED;Exploring Serverless Computing for Neural Network Training;Serverless or functions as a service runtimes have shown significant benefits to efficiency and cost for event-driven cloud applications. Although serverless runtimes are limited to applications requiring lightweight computation and memory, such as machine learning prediction and inference, they have shown improvements on these applications beyond other cloud runtimes. Training deep learning can be both compute and memory intensive. We investigate the use of serverless runtimes while leveraging data parallelism for large models, show the challenges and limitations due to the tightly coupled nature of such models, and propose modifications to the underlying runtime implementations that would mitigate them. For hyperparameter optimization of smaller deep learning models, we show that serverless runtimes can provide significant benefit. © 2018 IEEE.;Feng L., Kudva P., Da Silva D., Hu J.;2018
ACCEPTED;FAASM: Lightweight isolation for efficient stateful serverless computing;"Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms isolate functions in ephemeral, stateless containers, preventing them from directly sharing memory. This forces users to duplicate and serialise data repeatedly, adding unnecessary performance and resource costs. We believe that a new lightweight isolation approach is needed, which supports sharing memory directly between functions and reduces resource overheads. We introduce Faaslets, a new isolation abstraction for high-performance serverless computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, FAASM, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, FAASM restores Faaslets from already-initialised snapshots. We compare FAASM to a standard container-based platform and show that, when training a machine learning model, it achieves a 2_ speed-up with 10_ less memory; for serving machine learning inference, FAASM doubles the throughput and reduces tail latency by 90%. Copyright © Proc. of the 2020 USENIX Annual Technical Conference, ATC 2020. All rights reserved.";Shillaker S., Pietzuch P.;2020
REJECTED;FaaStest - Machine learning based cost and performance FaaS optimization;With the emergence of Function-as-a-Service (FaaS) in the cloud, pay-per-use pricing models became available along with the traditional fixed price model for VMs and increased the complexity of selecting the optimal platform for a given service. We present FaaStest - an autonomous solution for cost and performance optimization of FaaS services by taking a hybrid approach - learning the behavioral patterns of the service and dynamically selecting the optimal platform. Moreover, we combine a prediction based solution for reducing cold starts of FaaS services. Experiments present a reduction of over 50% in cost and over 90% in response time for FaaS calls. © 2019, Springer Nature Switzerland AG.;Horovitz S., Amos R., Baruch O., Cohen T., Oyar T., Deri A.;2019
ACCEPTED;FedLess: Secure and Scalable Federated Learning Using Serverless Computing;The traditional cloud-centric approach for Deep Learning (DL) requires training data to be collected and processed at a central server which is often challenging in privacy-sensitive domains like healthcare. Towards this, a new learning paradigm called Federated Learning (FL) has been proposed that brings the potential of DL to these domains while addressing privacy and data ownership issues. FL enables clients to learn a shared ML model while keeping the data local. However, conventional FL systems face challenges such as scalability, complex infrastructure management, and wasted compute and incurred costs due to idle clients. These challenges of FL systems closely align with the core problems that serverless computing and Function-as-a-Service (FaaS) platforms aim to solve. These include rapid scalability, no infrastructure management, automatic scaling to zero for idle clients, and a pay-per-use billing model. To this end, we present a novel system and framework for serverless FL, called FedLess. Our system supports multiple commercial and self-hosted FaaS providers and can be deployed in the cloud, on-premise in institutional data centers, and on edge devices. To the best of our knowledge, we are the first to enable FL across a large fabric of heterogeneous FaaS providers while providing important features like security and Differential Privacy. We demonstrate with comprehensive experiments that the successful training of DNNs for different tasks across up to 200 client functions and more is easily possible using our system. Furthermore, we demonstrate the practical viability of our methodology by comparing it against a traditional FL system and show that it can be cheaper and more resource-efficient. © 2021 IEEE.;Grafberger A., Chadha M., Jindal A., Gu J., Gerndt M.;2021
ACCEPTED;Gillis: Serving large neural networks in serverless functions with automatic model partitioning;The increased use of deep neural networks has stimulated the growing demand for cloud-based model serving platforms. Serverless computing offers a simplified solution: users deploy models as serverless functions and let the platform handle provisioning and scaling. However, serverless functions have constrained resources in CPU and memory, making them inefficient or infeasible to serve large neural networks-which have become increasingly popular. In this paper, we present Gillis, a serverless-based model serving system that automatically partitions a large model across multiple serverless functions for faster inference and reduced memory footprint per function. Gillis employs two novel model partitioning algorithms that respectively achieve latency-optimal serving and cost-optimal serving with SLO compliance. We have implemented Gillis on three serverless platforms-AWS Lambda, Google Cloud Functions, and KNIX-with MXNet as the serving backend. Experimental evaluations against popular models show that Gillis supports serving very large neural networks, reduces the inference latency substantially, and meets various SLOs with a low serving cost. © 2021 IEEE.;Yu M., Jiang Z., Ng H.C., Wang W., Chen R., Li B.;2021
REJECTED;GPU usage estimation of deep learning training function for serverless computing;Serverless computing is in the spotlight recently as a new form of cloud computing. And one of the most interested software domain in recent years is deep learning applications. Now serverless computing environment is still just CPU-based. This is because GPU devices are not shared by different processes at the same time unlike CPU. To support deep learning applications in serverless computing with low cost, it is essential to support GPU resource sharing. Nvidia supports MPS with execution resource provisioning on the latest Volta architecture GPU. In order to apply MPS and resource provisioning to GPU-based servlerless computing, it is necessary to know the accurate GPU usage of long-term deep learning functions. In this paper, we propose a technique to predict GPU usage of long-term deep learning training function without watching complete execution of it. The proposed technique is composed of sliding window method and coverage based usage estimation. Through the proposed technique, deep learning training functions can be effectively applied to serverless computing with GPU sharing. © 2018 IADIS Press. All Rights Reserved.;Kim C.-Y., Cha G.-I.;2018
ACCEPTED;High performance serverless architecture for deep learning workflows;Serverless architecture is a rapidly growing paradigm for deploying deep learning applications performing ephemeral computing and serving bursty workloads. Serverless architecture promises automatic scaling and cost efficiency for inferencing deep learning models while minimizing the operational logic. However, serverless computing is stateless with constraints on local resources. Hence, deploying complex deep learning applications containing large size models, frameworks, and libraries is a challenge.In this work, we discuss a methodology and architecture for migrating deep vision algorithms and model based applications to a serverless computing platform. We have tested our methodology using AWS infrastructure (AWS Lambda, Provisioned Concurrency, VPC endpoint, S3 and EFS) to mitigate the challenges in deploying composition of APIs containing large deep learning models and frameworks. We evaluate the performance and cost of our architecture for a real-life enterprise application used for document processing. © 2021 IEEE.;Chahal D., Ramesh M., Ojha R., Singhal R.;2021
ACCEPTED;Implementation of unsupervised k-means clustering algorithm within amazon web services lambda;This work demonstrates how an unsupervised learning algorithm based on k-Means Clustering with Kaufman Initialization may be implemented effectively as an Amazon Web Services Lambda Function, within their serverless cloud computing service. It emphasizes the need to employ a lean and modular design philosophy, transfer data efficiently between Lambda and DynamoDB, as well as employ Lambda Functions within mobile applications seamlessly and with negligible latency. This work presents a novel application of serverless cloud computing and provides specific examples that will allow readers to develop similar algorithms. The author provides compares the computation speed and cost of machine learning implementations on traditional PC and mobile hardware (running locally) as well as implementations that employ Lambda. © 2018 IEEE.;Deese A.;2018
ACCEPTED;Implications of Public Cloud Resource Heterogeneity for Inference Serving;We are witnessing an increasing trend towards using Machine Learning (ML) based prediction systems, spanning across different application domains, including product recommendation systems, personal assistant devices, facial recognition, etc. These applications typically have diverse requirements in terms of accuracy and response latency, that can be satisfied by a myriad of ML models. However, the deployment cost of prediction serving primarily depends on the type of resources being procured, which by themselves are heterogeneous in terms of provisioning latencies and billing complexity. Thus, it is strenuous for an inference serving system to choose from this confounding array of resource types and model types to provide low-latency and cost-effective inferences. In this work we quantitatively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource offerings. Our evaluation shows that, prior work does not solve the problem from both dimensions of model and resource heterogeneity. Hence, to holistically address this problem, we need to solve the issues that arise from combining both model and resource heterogeneity towards optimizing for application constraints. Towards this, we discuss the design implications of a self-managed inference serving system, which can optimize for application requirements based on public cloud resource characteristics. © 2020 ACM.;Gunasekaran J.R., Mishra C.S., Thinakaran P., Kandemir M.T., Das C.R.;2020
ACCEPTED;Leveraging the serverless paradigm for realizing machine learning pipelines across the edge-cloud continuum;"The exceedingly exponential-growing data rate highlighted numerous requirements and several approaches have been released to maximize the added-value of cloud and edge resources. Whereas data scientists utilize algorithmic models in order to transform datasets and extract actionable knowledge, a key challenge is oriented towards abstracting the underline layers: the ones enabling the management of infrastructure resources and the ones responsible to provide frameworks and components as services. In this sense, the serverless approach features as the novel paradigm of new cloud-related technology, enabling the agile implementation of applications and services. The concept of Function as a Service (FaaS) is introduced as a revolutionary model that offers the means to exploit serverless offerings. Developers have the potential to design their applications with the necessary scalability in the form of nanoservices without addressing themselves the way the infrastructure resources should be deployed and managed. By abstracting away the underlying hardware allocations, the data scientist concentrates on the business logic and critical problems of Machine Learning (ML) algorithms. This paper introduces an approach to realize the provision of ML Functions as a Service (i.e., ML-FaaS), by exploiting the Apache OpenWhisk event-driven, distributed serverless platform. The presented approach tackles also composite services that consist of single ones i.e., workflows of ML tasks including processes such as aggregation, cleaning, feature extraction, and analytics; thus, reflecting the complete data path. We also illustrate the operation of the approach mentioned above and assess its performance and effectiveness exploiting a holistic, end-toend anti-fraud detection machine learning pipeline. © 2021 IEEE.";Paraskevoulakou E., Kyriazis D.;2021
ACCEPTED;Mark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving;The advances of Machine Learning (ML) have sparked a growing demand of ML-as-a-Service: developers train ML models and publish them in the cloud as online services to provide low-latency inference at scale. The key challenge of ML model serving is to meet the response-time Service-Level Objectives (SLOs) of inference workloads while minimizing the serving cost. In this paper, we tackle the dual challenge of SLO compliance and cost effectiveness with MArk (Model Ark), a general-purpose inference serving system built in Amazon Web Services (AWS). MArk employs three design choices tailor-made for inference workload. First, MArk dynamically batches requests and opportunistically serves them using expensive hardware accelerators (e.g., GPU) for improved performance-cost ratio. Second, instead of relying on feedback control scaling or over-provisioning to serve dynamic workload, which can be too slow or too expensive for inference serving, MArk employs predictive autoscaling to hide the provisioning latency at low cost. Third, given the stateless nature of inference serving, MArk exploits the flexible, yet costly serverless instances to cover the occasional load spikes that are hard to predict. We evaluated the performance of MArk using several state-of-the-art ML models trained in popular frameworks including TensorFlow, MXNet, and Keras. Compared with the premier industrial ML serving platform SageMaker, MArk reduces the serving cost up to 7.8_ while achieving even better latency performance. © Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019. All rights reserved.;Zhang C., Yu M., Wang W., Yan F.;2019
ACCEPTED;Migrating Large Deep Learning Models to Serverless Architecture;Serverless computing platform is emerging as a solution for event-driven artificial intelligence applications. Function-as-a-Service (FaaS) using serverless computing paradigm provides high performance and low cost solutions for deploying such applications on cloud while minimizing the operational logic. Using FaaS for efficient deployment of complex applications, such as natural language processing (NLP) and image processing, containing large deep learning models will be an advantage. However, constrained resources and stateless nature of FaaS offers numerous challenges while deploying such applications. In this work, we discuss the methodological suggestions and their implementation for deploying pre-trained large size machine learning and deep learning models on FaaS. We also evaluate the performance and deployment cost of an enterprise application, consisting of suite of deep vision preprocessing algorithms and models, on VM and FaaS platform. Our evaluation shows that migration from monolithic to FaaS platform significantly improves the performance of the application at a reduced cost. © 2020 IEEE.;Chahal D., Ojha R., Ramesh M., Singhal R.;2020
ACCEPTED;OFC: An opportunistic caching system for FaaS platforms;"Cloud applications based on the ""Functions as a Service""(FaaS) paradigm have become very popular. Yet, due to their stateless nature, they must frequently interact with an external data store, which limits their performance. To mitigate this issue, we introduce OFC, a transparent, vertically and horizontally elastic in-memory caching system for FaaS platforms, distributed over the worker nodes. OFC provides these benefits cost-effectively by exploiting two common sources of resource waste: (i) most cloud tenants overprovision the memory resources reserved for their functions because their footprint is non-trivially input-dependent and (ii) FaaS providers keep function sandboxes alive for several minutes to avoid cold starts. Using machine learning models adjusted for typical function input data categories (e.g., multimedia formats), OFC estimates the actual memory resources required by each function invocation and hoards the remaining capacity to feed the cache. We build our OFC prototype based on enhancements to the OpenWhisk FaaS platform, the Swift persistent object store, and the RAM-Cloud in-memory store. Using a diverse set of workloads, we show that OFC improves by up to 82 % and 60 % respectively the execution time of single-stage and pipelined functions. © 2021 ACM.";Mvondo D., Bacou M., Nguetchouang K., Ngale L., Pouget S., Kouam J., Lachaize R., Hwang J., Wood T., Hagimont D., De Palma N., Batchakui B., Tchana A.;2021
ACCEPTED;On the FaaS track: Building stateful distributed applications with serverless architectures;Serverless computing is an emerging paradigm that greatly simplifies the usage of cloud resources and suits well to many tasks. Most notably, Function-as-a-Service (FaaS) enables programmers to develop cloud applications as individual functions that can run and scale independently. Yet, due to the disaggregation of storage and compute resources in FaaS, applications that require fine-grained support for mutable state and synchronization, such as machine learning and scientific computing, are hard to build. In this work, we present Crucial, a system to program highly-concurrent stateful applications with serverless architectures. Its programming model keeps the simplicity of FaaS and allows to port effortlessly multi-threaded algorithms to this new environment. Crucial is built upon the key insight that FaaS resembles to concurrent programming at the scale of a data center. As a consequence, a distributed shared memory layer is the right answer to the need for fine-grained state management and coordination in serverless. We validate our system with the help of micro-benchmarks and various applications. In particular, we implement two common machine learning algorithms: k-means clustering and logistic regression. For both cases, Crucial obtains superior or comparable performance to an equivalent Spark cluster. © 2019 Association for Computing Machinery.;Barcelona-Pons D., Sánchez-Artigas M., París G., Sutra P., García-López P.;2019
REJECTED;Optimized container scheduling for data-intensive serverless edge computing;Operating data-intensive applications on edge systems is challenging, due to the extreme workload and device heterogeneity, as well as the geographic dispersion of compute and storage infrastructure. Serverless computing has emerged as a compelling model to manage the complexity of such systems, by decoupling the underlying infrastructure and scaling mechanisms from applications. Although serverless platforms have reached a high level of maturity, we have found several limiting factors that inhibit their use in an edge setting. This paper presents a container scheduling system that enables such platforms to make efficient use of edge infrastructures. Our scheduler makes heuristic trade-offs between data and computation movement, and considers workload-specific compute requirements such as GPU acceleration. Furthermore, we present a method to automatically fine-tune the weights of scheduling constraints to optimize high-level operational objectives such as minimizing task execution time, uplink usage, or cloud execution cost. We implement a prototype that targets the container orchestration system Kubernetes, and deploy it on an edge testbed we have built. We evaluate our system with trace-driven simulations in different infrastructure scenarios, using traces generated from running representative workloads on our testbed. Our results show that (a) our scheduler significantly improves the quality of task placement compared to the state-of-the-art scheduler of Kubernetes, and (b) our method for fine-tuning scheduling parameters helps significantly in meeting operational goals. © 2020;Rausch T., Rashed A., Dustdar S.;2021
REJECTED;Optimizing serverless computing: Introducing an adaptive function placement algorithm;"The main concept behind serverless computing is to build and run applications without the need for server management. It refers to a fine-grained deployment model where applications, comprising of one or more functions, are uploaded to a platform and then executed, scaled, and billed in response to the exact demand needed at the moment. While elite cloud vendors such as Amazon, Google, Microsoft, and IBM are now providing serverless computing, their approach for the placement of functions, i.e. associated container or sandbox, on servers is oblivious to the workload which may lead to poor performance and/or higher operational cost for software owners. In this paper, using statistical machine learning, we design and evaluate an adaptive function placement algorithm which can be used by serverless computing platforms to optimize the performance of running functions while minimizing the operational cost. Given a fixed amount of resources, our smart spread function placement algorithm results in higher performance compared to existing approaches; this will be achieved by maintaining the users' desired quality of service for a longer time which prevents premature scaling of the cloud resources. Extensive experimental studies revealed that the proposed adaptive function placement algorithm can be easily adopted by serverless computing providers and integrated to container orchestration platforms without introducing any limiting side effects. © 2019 Copyright held by the owner/author(s).";Mahmoudi N., Khazaei H., Lin C., Litoiu M.;2020
ACCEPTED;Performance and cost comparison of cloud services for deep learning workload;Many organizations are migrating their on-premise artificial intelligence workloads to the cloud due to availability of cost-effective and highly scalable infrastructure, software and platform services. To ease the process of migration, many cloud vendors provide services, frameworks and tools that can be used for deployment of applications on cloud infrastructure. Finding the most appropriate service and infrastructure for a given application that results in a desired performance at minimal cost, is a challenge. In this work, we present a methodology to migrate a deep learning model based recommender system to ML platform and serverless architecture. Furthermore, we show our experimental evaluation of AWS ML platform called SageMaker and the serverless platform service known as Lambda. In our study, we also discuss performance and cost trade-off while using cloud infrastructure. © 2021 Association for Computing Machinery.;Chahal D., Mishra M., Palepu S., Singhal R.;2021
ACCEPTED;Performance Characterization and Modeling of Serverless and HPC Streaming Applications;Industrial and scientific streaming applications require support for different types of processing and the management of heterogeneous infrastructure over a dynamic range of scales: from the edge to the cloud and HPC, and intermediate resources. Serverless is an emerging service that combines high-level middleware services, such as distributed execution engines for managing tasks, with low-level infrastructure. It offers the potential of usability and scalability but adds to the complexity of managing heterogeneous and dynamic resources. In response, we extend Pilot-Streaming to support serverless platforms. Pilot-Streaming provides a unified abstraction for resource management for HPC, cloud, and serverless, and allocates resource containers independent of the application workload removing the need to write resource-specific code. Understanding the performance and scaling characteristics of streaming applications and infrastructure presents another challenge. StreamInsight provides insight into the performance of streaming applications and infrastructure, their selection, configuration, and scaling behavior. Underlying StreamInsight is the universal scalability law, which permits the accurate quantification of scalability properties of streaming applications. Using experiments on HPC and AWS Lambda, we demonstrate that StreamInsight provides an accurate model for a variety of application characteristics, e. g., machine learning model sizes and resource configurations. © 2019 IEEE.;Luckow A., Jha S.;2019
REJECTED;Potential Bottleneck and Measuring Performance of Serverless Computing: A Literature Study;Trending form of cloud computing is Serverless computing, where developer just needs to focus on his code rather than worrying about server management. In serverless computing, application is nothing but collection of one or more functions, written for specific business functionality, which triggers on an event. There are various cloud service providers, i.e. Amazon, Microsoft, Google, IBM, etc. who provide serverless services, on pay as you use and auto scalable solution to execute the application code as a function. The developer just needs to upload the code for execution. The performance of the serverless computing may vary due to dynamic configuration of the solution, technologies and different technology used by the service provider.This paper reviews various past and recent work in the serverless computing to identify possible bottlenecks and the scope of measuring performance of serverless computing. It will also put some light to leverage machine learning in various possible ways to do performance engineering for future research. © 2020 IEEE.;Khatri D., Khatri S.K., Mishra D.;2020
ACCEPTED;Prognostics by classifying degradation stage on lambda architecture;To enhance the reliability and availability of an asset in its life, predicting the remaining useful life of an asset is strongly encouraged by assessing the extent of deviation or degradation of the asset's monitored parameters from its expected normal operating conditions. Although intelligent fault prognostic techniques such as machine learning and artificial neural networks have been applied in modern industries, application in actual industrial conditions requires that the forecasting process is revealed and more descriptive. To investigate the issue and increase the accuracy, this paper proposes an additional technique that can be further applied to any recent intelligent prognostic methods. The proposed method consists of two steps. First, the entire training set is divided into several degradation stages before regression using a heuristic approach and then the regression results are synthesized for each stage. The proposed method will increase the monotonicity of the predictive parameters, thus helping improve the predictive model's accuracy. To demonstrate the hypothesis, real condition monitoring data of high-pressure LNG pump and acceleration experimental data of a rotating machine is used for an experiment. Moreover, a system in which the proposed method can be appropriately executed is introduced with Lambda architecture. Finally, by demonstrating that the proposed method is capable of parallel computing, it is proven suitable for use in the proposed large-scale distributed processing system. © 2020 IEEE.;Choi J., Lee J., Cho W.J.;2020
ACCEPTED;Quantifying and Improving Performance of Distributed Deep Learning with Cloud Storage;Cloud computing provides a powerful yet low-cost environment for distributed deep learning workloads. However, training complex deep learning models often requires accessing large amounts of data, which can easily exceed the capacity of local disks. Prior research often overlooks this training data problem by implicitly assuming that data is available locally or via low latency network-based data storage. Such implicit assumptions often do not hold in a cloud-based training environment, where deep learning practitioners create and tear down dedicated GPU clusters on demand, or do not have the luxury of local storage, such as in serverless workloads. In this work, we investigate the performance of distributed training that leverages training data residing entirely inside cloud storage buckets. These buckets promise low storage costs, but come with inherent bandwidth limitations that make them seem unsuitable for an efficient training solution. To account for these bandwidth limitations, we propose the use of two classical techniques, namely caching and pre-fetching, to mitigate the training performance degradation. We implement a prototype, DELI, based on the popular deep learning framework PyTorch by building on its data loading abstractions. We then evaluate the training performance of two deep learning workloads using Google Cloud's NVIDIA K80 GPU servers and show that we can reduce the time that the training loop is waiting for data by 85.6%-93.5% compared to loading directly from a storage bucket-thus achieving comparable performance to loading data directly from disk-while only storing a fraction of the data locally at a time. In addition, DELI has the potential of lowering the cost of running a training workload, especially on models with long per-epoch training times. © 2021 IEEE.;Krichevsky N., St Louis R., Guo T.;2021
ACCEPTED;Refactoring of Neural Network Models for Hyperparameter Optimization in Serverless Cloud;Machine Learning and Neural Networks in particular have become hot topics in Computer Science. The recent 2019 Turing award to the forefathers of Deep Learning and AI - Yoshua Bengio, Geoffrey Hinton, and Yann LeCun proves the importance of the technology and its effect on science and industry. However, we have realized that even nowadays, the state of the art methods require several manual steps for neural network hyperparameter optimization. Our approach automates the model tuning by refactoring the original Python code using open-source libraries for processing. We were able to identify hyperparameters by parsing the original source and analyzing it. Given these parameters, we refactor the model, add the state of the art optimization library calls, and run the updated code in the Serverless Cloud. Our approach has proven to eliminate manual steps for an arbitrary TensorFlow and Keras tuning. We have created a tool called OptPar which automatically refactors an arbitrary Deep Neural Network optimizing its hyperparameters. Such a transformation can save hours of time for Data Scientists, giving them an opportunity to concentrate on designing their Machine Learning algorithms. © 2020 ACM.;Kaplunovich A., Yesha Y.;2020
ACCEPTED;Reinforcement learning-assisted autoscaling mechanisms for serverless computing platforms;Serverless computing is emerging as a cloud computing paradigm that provisions computing resources on demand, while billing is taking place based on the exact usage of the cloud resources. The responsibility for infrastructure management is undertaken by cloud providers, enabling developers to focus on the development of the business logic of their applications. For managing scalability, various autoscaling mechanisms have been proposed that try to optimize the provisioning of resources based on the posed workload. These mechanisms are configured and managed by the cloud provider, imposing non negligible administration overhead. A set of challenges are identified for introducing automation and optimizing the provisioning of resources, while in parallel respecting the agreed Service Level Agreement between cloud and application providers. To address these challenges, we have developed autoscaling mechanisms for serverless applications that are powered by Reinforcement Learning (RL) techniques. A set of RL environments and agents have been implemented (based on Q-learning, DynaQ+ and Deep Q-learning algorithms) for driving autoscaling mechanisms, able to autonomously manage dynamic workloads with Quality of Service (QoS) guarantees, while opting for efficient usage of resources. The produced environments and agents are evaluated in real and simulated environments, taking advantage of the Kubeless open-source serverless platform. The evaluation results validate the suitability of the proposed mechanisms to efficiently tackle scalability management for serverless applications. © 2022 Elsevier B.V.;Zafeiropoulos A., Fotopoulou E., Filinis N., Papavassiliou S.;2022
REJECTED;Reputation-aware Hedonic Coalition Formation for Efficient Serverless Hierarchical Federated Learning;Amid growing concerns on data privacy, Federated Learning (FL) has emerged as a promising privacy preserving distributed machine learning paradigm. Given that the FL network is expected to be implemented at scale, several studies have proposed system architectures towards improving the network scalability and efficiency. Specifically, the Hierarchical FL (HFL) network utilizes cluster heads, e.g., base stations, for the intermediate aggregation and relay of model parameters. Serverless FL is also proposed recently, in which the data owners, i.e., workers, exchange the local model parameters among a neighborhood of workers. This decentralized approach reduces the risk of a single point of failure but inevitably incurs significant communication overheads. To achieve the best of both worlds, we propose the Serverless Hierarchical Federated Learning (SHFL) framework in this paper. The SHFL framework adopts a two-layer system architecture. In the lower layer, the FL workers are grouped into clusters under cluster heads. In the upper layer, the cluster heads exchange the intermediate parameters with their one-hop neighbors without the aid of a central server. To improve the sustainable efficiency of the FL system while taking into account the incentive design for workers marginal contributions in the system, we propose the reputation-aware hedonic coalition formation game in this paper. Specifically, the workers are rewarded for their marginal contribution to the cluster, whereas the reputation opinions of each cluster head is updated in a decentralized manner, thereby deterring malicious behaviors by the cluster head. This improves the performance of the network since cluster heads with higher reputation scores are more reliable in relaying the intermediate model parameters. The simulation results show that our proposed hedonic coalition formation algorithm converges to a Nash-stable partition and improves the network efficiency. IEEE;Ng J.S., Lim W.Y.B., Xiong Z., Cao X., Jin J., Niyato D., Leung C.S., Miao C.;2021
ACCEPTED;Seneca: Fast and low cost hyperparameter search for machine learning models;The goal of our work is to simplify and expedite the construction and evaluation of machine learning models using autoscaled cloud computing resources. To enable this, we develop an open source system called Seneca, which leverages the serverless programming model and its implementation in Amazon Web Services (AWS) Lambda. Seneca takes a machine learning application, dataset, and a list of possible hyperparameter options as input and automatically constructs an AWS Lambda function. The function ingresses and splits the input dataset into training and testing subsets and constructs, tests, and evaluates (i.e. scores) a machine learning model for a given set of hyperparameter values. Seneca concurrently invokes functions for all combinations of the hyperparameters specified. It then returns the configuration (or model) that results in the best score to the user. In this paper, we overview the design and implementation of Seneca, and empirically evaluate its performance for a popular classification application. © 2019 IEEE.;Zhang M., Krintz C., Mock M., Wolski R.;2019
ACCEPTED;Serverless Computing Approach for Deploying Machine Learning Applications in Edge Layer;Serverless computing-a stateless cloud computing model, is an emerging solution that has shown significant benefits to efficiency and cost for event-driven applications in the cloud environment, including artificial intelligence (AI), machine learning applications. With serverless computing, the machine learning system's complexity is minimized, flexible and straightforward in management. However, operating and managing serverless machine learning services on clouds faces many limitations such as latency and data privacy. Local distributed edge computing nodes which are closed to users can address these challenges of cloud-serverless AI applications. Based on this motivation, in this paper, we propose an architecture for deploying machine learning workload as serverless functions in the edge environment. We illustrate our proposed approach and evaluate its performance and effectiveness by exploiting a holistic end-to-end image classifier, a famous machine learning use case in the MNIST dataset. Our proof of concept provides comprehensive assessments that prove its effectiveness in latency reduction and distributed machine learning deployment. © 2022 IEEE.;Bac T.P., Tran M.N., Kim Y.;2022
ACCEPTED;Serving deep learning models in a serverless platform;Serverless computing has emerged as a compelling paradigm for the development and deployment of a wide range of event based cloud applications. At the same time, cloud providers and enterprise companies are heavily adopting machine learning and Artificial Intelligence to either differentiate themselves, or provide their customers with value added services. In this work we evaluate the suitability of a serverless computing environment for the inferencing of large neural network models. Our experimental evaluations are executed on the AWS Lambda environment using the MxNet deep learning framework. Our experimental results show that while the inferencing latency can be within an acceptable range, longer delays due to cold starts can skew the latency distribution and hence risk violating more stringent SLAs. © 2018 IEEE.;Ishakian V., Muthusamy V., Slominski A.;2018
ACCEPTED;Serving machine learning workloads in resource constrained environments: A serverless deployment example;"Deployed AI platforms typically ship with bulky system architectures which present bottlenecks and a high risk of failure. A serverless deployment can mitigate these factors and provide a cost-effective, automatically scalable (up or down) and elastic real-time on-demand AI solution. However, deploying high complexity production workloads into serverless environments is far from trivial, e.g., due to factors such as minimal allowance for physical codebase size, low amount of runtime memory, lack of GPU support and a maximum runtime before termination via timeout. In this paper we propose a set of optimization techniques and show how these transform a codebase which was previously incompatible with a serverless deployment into one that can be successfully deployed in a serverless environment; without compromising capability or performance. The techniques are illustrated via worked examples that have been deployed live on rail data and realtime predictions on train movements on the UK rail network. The similarities of a serverless environment to other resource constrained environments (IoT, Mobile) means the techniques can be applied to a range of use cases. © 2019 IEEE.";Christidis A., Davies R., Moschoyiannis S.;2019
REJECTED;SLA for Sequential Serverless Chains: A Machine Learning Approach;Despite its vast potential, a challenge facing serverless computing's wide-scale adoption is the lack of Service Level Agreements (SLAs) for serverless platforms. This challenge is compounded when composition technologies are employed to construct large applications using chains of functions. Due to the dependency of a chain's performance on each function forming it, a single function's sub-optimal performance can result in performance degradations of the entire chain. This paper sheds light on this problem and provides a categorical classification of the factors that impact a serverless function execution performance. We discuss the challenge of serverless chains' SLA and present the results of leveraging FaaS2F, our proposed serverless SLA framework, to define SLAs for fixed-size and variable-size sequential serverless chains. The validation results demonstrate high accuracy in detecting sub-optimal executions exceeding 79%. © 2021 ACM.;Elsakhawy M., Bauer M.;2021
ACCEPTED;SLA-Aware Workload Scheduling Using Hybrid Cloud Services;Cloud services have an auto-scaling feature for load balancing to meet the performance requirements of an application. Existing auto-scaling techniques are based on upscaling and downscaling cloud resources to distribute the dynamically varying workloads. However, bursty workloads pose many challenges for auto-scaling and sometimes result in Service Level Agreement (SLA) violations. Furthermore, over-provisioning or under-provisioning cloud resources to address dynamically evolving workloads results in performance degradation and cost escalation. In this work, we present a workload characterization based approach for scheduling the bursty workload on a highly scalable serverless architecture in conjunction with a machine learning (ML) platform. We present the use of Amazon Web Services (AWS) ML platform SageMaker and serverless computing platform Lambda for load balancing the inference workload to avoid SLA violations. We evaluate our approach using a recommender system that is based on a deep learning model for inference. © 2020 ACM.;Chahal D., Palepu S., Mishra M., Singhal R.;2021
ACCEPTED;STOIC: Serverless Teleoperable Hybrid Cloud for Machine Learning Applications on Edge Device;Serverless computing is a promising new event-driven programming model that was designed by cloud vendors to expedite the development and deployment of scalable web services on cloud computing systems. Using the model, developers write applications that consist of simple, independent, stateless functions that the cloud invokes on-demand (i.e. elastically), in response to system-wide events (data arrival, messages, web requests, etc.). In this work, we present STOIC (Serverless TeleOperable HybrId Cloud), an application scheduling and deployment system that extends the serverless model in two ways. First, it uses the model in a distributed setting and schedules application functions across multiple cloud systems. Second, STOIC supports serverless function execution using hardware acceleration (e.g. GPU resources) when available from the underlying cloud system. We overview the design and implementation of STOIC and empirically evaluate it using real-world machine learning applications and multi-tier (e.g. edge-cloud) deployments. We find that STOIC's combined use of edge and cloud resources is able to outperform using either cloud in isolation for the applications and datasets that we consider. © 2020 IEEE.;Zhang M., Krintz C., Wolski R.;2020
ACCEPTED;Stratum: A serverless framework for the lifecycle management of machine learning-based data analytics tasks;With the proliferation of machine learning (ML) libraries and frameworks, and the programming languages that they use, along with operations of data loading, transformation, preparation and mining, ML model development is becoming a daunting task. Furthermore, with a plethora of cloud-based ML model development platforms, heterogeneity in hardware, increased focus on exploiting edge computing resources for low-latency prediction serving and often a lack of a complete understanding of resources required to execute ML workflows efficiently, ML model deployment demands expertise for managing the lifecycle of ML workflows efficiently and with minimal cost. To address these challenges, we propose an end-to-end data analytics, a serverless platform called Stratum. Stratum can deploy, schedule and dynamically manage data ingestion tools, live streaming apps, batch analytics tools, ML-as-a-service (for inference jobs), and visualization tools across the cloud-fog-edge spectrum. This paper describes the Stratum architecture highlighting the problems it resolves.;Bhattacharjee A., Barve Y., Khare S., Bao S., Gokhale A., Damiano T.;2019
ACCEPTED;Toward Sustainable Serverless Computing;"Although serverless computing generally involves executing short-lived functions, the increasing migration to this computing paradigm requires careful consideration of energy and power requirements. serverless computing is also viewed as an economically-driven computational approach, often influenced by the cost of computation, as users are charged for per-subsecond use of computational resources rather than the coarse-grained charging that is common with virtual machines and containers. To ensure that the startup times of serverless functions do not discourage their use, resource providers need to keep these functions hot, often by passing in synthetic data. We describe the real power consumption characteristics of serverless, based on execution traces reported in the literature, and describe potential strategies (some adopted from existing VM and container-based approaches) that can be used to reduce the energy overheads of serverless execution. Our analysis is, purposefully, biased toward the use of machine learning workloads because: (1) workloads are increasingly being used widely across different applications; (2) functions that implement machine learning algorithms can range in complexity from long-running (deep learning) versus short-running (inference only), enabling us to consider serverless across a variety of possible execution behaviors. The general findings are easily translatable to other domains. © 1997-2012 IEEE.";Patros P., Spillner J., Papadopoulos A.V., Varghese B., Rana O., Dustdar S.;2021
ACCEPTED;Towards Demystifying Serverless Machine Learning Training;"The appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive applications such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their performance and relative advantage over ""serverful""infrastructures (IaaS). In this paper we present a systematic, comparative study of distributed ML training over FaaS and IaaS. We present a design space covering design choices such as optimization algorithms and synchronization protocols, and implement a platform, LambdaML, that enables a fair comparison between FaaS and IaaS. We present experimental results using LambdaML, and further develop an analytic model to capture cost/performance tradeoffs that must be considered when opting for a serverless infrastructure. Our results indicate that ML training pays off in serverless only for models with efficient (i.e., reduced) communication and that quickly converge. In general, FaaS can be much faster but it is never significantly cheaper than IaaS. © 2021 ACM.";Jiang J., Gan S., Liu Y., Wang F., Alonso G., Klimovic A., Singla A., Wu W., Zhang C.;2021
ACCEPTED;Towards Federated Learning using FaaS Fabric;Federated learning (FL) enables resource-constrained edge devices to learn a shared Machine Learning (ML) or Deep Neural Network (DNN) model, while keeping the training data local and providing privacy, security, and economic benefits. However, building a shared model for heterogeneous devices such as resource-constrained edge and cloud makes the efficient management of FL-clients challenging. Furthermore, with the rapid growth of FL-clients, the scaling of FL training process is also difficult. In this paper, we propose a possible solution to these challenges: federated learning over a combination of connected Function-as-a-Service platforms, i.e., FaaS fabric offering a seamless way of extending FL to heterogeneous devices. Towards this, we present FedKeeper, a tool for efficiently managing FL over FaaS fabric. We demonstrate the functionality of FedKeeper by using three FaaS platforms through an image classification task with a varying number of devices/clients, different stochastic optimizers, and local computations (local epochs). © 2020 ACM.;Chadha M., Jindal A., Gerndt M.;2020
ACCEPTED;Towards situational awareness with multimodal streaming data fusion: Serverless computing approach;The availability of large quantities of data has given an impulse for methods and techniques to extract unseen useful knowledge and process it in a fast and scalable manner. In order to extract the most complete possible knowledge from the continuous data stream, it is necessary to use the heterogeneous data sources and process information from multiple modalities. The systems that utilize multimodal data must take advantage of the up-To-date approaches for data storage, usage, cleaning and storage. Neural networks and machine learning approaches are widely used for data-heavy software where pattern extractions and predictions need to be conducted while serverless computing frameworks are being increasingly used for machine learning solutions to optimize cost and speed of such systems. This work presents a framework for processing data from multimodal sources where the task of feature and pattern extraction is performed on a serverless computing platform. The use cases for public safety solutions to increase situational awareness are described and compared with other implementation approaches. © 2021 Owner/Author.;Nesen A., Bhargava B.;2021
ACCEPTED;TrIMS: Transparent and isolated model sharing for low latency deep learning inference in function-as-a-service;Deep neural networks (DNNs) have become core computation components within low latency Function as a Service (FaaS) prediction pipelines. Cloud computing, as the defacto backbone of modern computing infrastructure, has to be able to handle user-defined FaaS pipelines containing diverse DNN inference workloads while maintaining isolation and latency guarantees with minimal resource waste. The current solution for guaranteeing isolation and latency within FaaS is inefficient. A major cause of the inefficiency is the need to move large amount of data within and across servers. We propose TrIMS as a novel solution to address this issue. TrIMSis a generic memory sharing technique that enables constant data to be shared across processes or containers while still maintaining isolation between users. TrIMS consists of a persistent model store across the GPU, CPU, local storage, and cloud storage hierarchy, an efficient resource management layer that provides isolation, and a succinct set of abstracts, applicationAPIs, and container technologies for easy and transparent integration with FaaS, Deep Learning (DL) frameworks, and user code. We demonstrate our solution by interfacing TrIMS with the Apache MXNet framework and demonstrate up to 24x speedup in latency for image classification models, up to 210x speedup for large models, and up to8_system throughput improvement. © 2019 IEEE.;Dakkak A., Li C., De Gonzalo S.G., Xiong J., Hwu W.-M.;2019
ACCEPTED;WidePipe: High-Throughput Deep Learning Inference System on a Cluster of Neural Processing Units;The wide application of machine learning technology promotes the generation of ML-as-a-Service(MLaaS), which is a serverless computing paradigm for rapidly deploying a trained model as a serving. However, it is a challenge to design an inference system that is capable of coping with large traffic for low latency and heterogeneous neural networks. It is difficult to adaptively configure multilevel parallelism in existing cloud inference systems for machine learning servings, particularly if the cluster has accelerators, such as GPUs, NPUs, FPGAs, etc. These issues lead to poor resource utilization and limit the system throughput. In this paper, we propose and implement a high-throughput inference system called WidePipe, which WidePipe leverages reinforcement learning to co-adapt resource allocation and batch size of request according to device status. We evaluated the performance of WidePipe for a large cluster with 1000 neural processing units in 250 nodes. Our experimental results show that WidePipe has a 2.11_ higher throughput than current inference systems when deploying heterogeneous machine learning servings, meeting the service-level objectives for the response time. © 2021 IEEE.;Ma L., Shao E., Zhou Y., Tan G.;2021
ACCEPTED;You Do Not Need a bigger boat: Recommendations at Reasonable Scale in a (Mostly) serverless and open stack;"We argue that immature data pipelines are preventing a large portion of industry practitioners from leveraging the latest research on recommender systems. We propose our template data stack for machine learning at ""reasonable scale"", and show how many challenges are solved by embracing a serverless paradigm. Leveraging our experience, we detail how modern open source tools can provide a pipeline processing terabytes of data with minimal infrastructure work. © 2021 Owner/Author.";Tagliabue J.;2021
;Reputation-Aware Hedonic Coalition Formation for Efficient Serverless Hierarchical Federated Learning;Amid growing concerns on data privacy, Federated Learning (FL) has emerged as a promising privacy preserving distributed machine learning paradigm. Given that the FL network is expected to be implemented at scale, several studies have proposed system architectures towards improving the network scalability and efficiency. Specifically, the Hierarchical FL (HFL) network utilizes cluster heads, e.g., base stations, for the intermediate aggregation and relay of model parameters. Serverless FL is also proposed recently, in which the data owners, i.e., workers, exchange the local model parameters among a neighborhood of workers. This decentralized approach reduces the risk of a single point of failure but inevitably incurs significant communication overheads. To achieve the best of both worlds, we propose the Serverless Hierarchical Federated Learning (SHFL) framework in this article. The SHFL framework adopts a two-layer system architecture. In the lower layer, the FL workers are grouped into clusters under cluster heads. In the upper layer, the cluster heads exchange the intermediate parameters with their one-hop neighbors without the aid of a central server. To improve the sustainable efficiency of the FL system while taking into account the incentive design for workers' marginal contributions in the system, we propose the reputation-aware hedonic coalition formation game in this article. Specifically, the workers are rewarded for their marginal contribution to the cluster, whereas the reputation opinions of each cluster head is updated in a decentralized manner, thereby deterring malicious behaviors by the cluster head. This improves the performance of the network since cluster heads with higher reputation scores are more reliable in relaying the intermediate model parameters. The simulation results show that our proposed hedonic coalition formation algorithm converges to a Nash-stable partition and improves the network efficiency. © 1990-2012 IEEE.;Ng J.S., Lim W.Y.B., Xiong Z., Cao X., Jin J., Niyato D., Leung C., Miao C.;2022
;Stateful Serverless Computing with Crucial;Serverless computing greatly simplifies the use of cloud resources. In particular, Function-As-A-Service (FaaS) platforms enable programmers to develop applications as individual functions that can run and scale independently. Unfortunately, applications that require fine-grained support for mutable state and synchronization, such as machine learning (ML) and scientific computing, are notoriously hard to build with this new paradigm. In this work, we aim at bridging this gap. We present Crucial, a system to program highly-parallel stateful serverless applications. Crucial retains the simplicity of serverless computing. It is built upon the key insight that FaaS resembles to concurrent programming at the scale of a datacenter. Accordingly, a distributed shared memory layer is the natural answer to the needs for fine-grained state management and synchronization. Crucial allows to port effortlessly a multi-Threaded code base to serverless, where it can benefit from the scalability and pay-per-use model of FaaS platforms. We validate Crucial with the help of micro-benchmarks and by considering various stateful applications. Beyond classical parallel tasks (e.g., a Monte Carlo simulation), these applications include representative ML algorithms such as k-means and logistic regression. Our evaluation shows that Crucial obtains superior or comparable performance to Apache Spark at similar cost (18%-40% faster). We also use Crucial to port (part of) a state-of-The-Art multi-Threaded ML library to serverless. The ported application is up to 30% faster than with a dedicated high-end server. Finally, we attest that Crucial can rival in performance with a single-machine, multi-Threaded implementation of a complex coordination problem. Overall, Crucial delivers all these benefits with less than 6% of changes in the code bases of the evaluated applications. © 2022 Copyright held by the owner/author(s).;Barcelona-Pons D., Sutra P., S·nchez-Artigas M., ParÌs G., GarcÌa-LÛpez P.;2022
;IFaaSBus: A Security- and Privacy-Based Lightweight Framework for Serverless Computing Using IoT and Machine Learning;As data of COVID-19 patients is increasing, the new framework is required to secure the data collected from various Internet of Things (IoT) devices and predict the trend of disease to reduce its spreading. This article proposes security- and privacy-based lightweight framework called iFaaSBus, which uses the concept of IoT, machine learning (ML), and function as a service (FaaS) or serverless computing to diagnose the COVID-19 disease and manages resources automatically to enable dynamic scalability. iFaaSBus offers OAuth-2.0 Authorization protocol-based privacy and JSON Web Token & Transport Layer Socket protocol-based security to secure the patient's health data. iFaaSBus outperforms response time compared to nonserverless computing while responding to up to 1100 concurrent requests. Further, the performance of various ML models is evaluated based on accuracy, precision, recall, F-score, and area under the curve (AUC) values, and the K-nearest neighbor model gives the highest accuracy rate of 97.51%. © 2005-2012 IEEE.;Golec M., Ozturac R., Pooranian Z., Gill S.S., Buyya R.;2022
;Accelerating Serverless Computing by Harvesting Idle Resources;Serverless computing automates fine-grained resource scaling and simplifies the development and deployment of online services with stateless functions. However, it is still non-trivial for users to allocate appropriate resources due to various function types, dependencies, and input sizes. Misconfiguration of resource allocations leaves functions either under-provisioned or over-provisioned and leads to continuous low resource utilization. This paper presents Freyr, a new resource manager (RM) for serverless platforms that maximizes resource efficiency by dynamically harvesting idle resources from over-provisioned functions to under-provisioned functions. Freyr monitors each function's resource utilization in real-time, detects over-provisioning and under-provisioning, and learns to harvest idle resources safely and accelerates functions efficiently by applying deep reinforcement learning algorithms along with a safeguard mechanism. We have implemented and deployed a Freyr prototype in a 13-node Apache OpenWhisk cluster. Experimental results show that 38.8% of function invocations have idle resources harvested by Freyr, and 39.2% of invocations are accelerated by the harvested resources. Freyr reduces the 99th-percentile function response latency by 32.1% compared to the baseline RMs. © 2022 ACM.;Yu H., Wang H., Li J., Yuan X., Park S.-J.;2022
;Reinforcement learning-assisted autoscaling mechanisms for serverless computing platforms;Serverless computing is emerging as a cloud computing paradigm that provisions computing resources on demand, while billing is taking place based on the exact usage of the cloud resources. The responsibility for infrastructure management is undertaken by cloud providers, enabling developers to focus on the development of the business logic of their applications. For managing scalability, various autoscaling mechanisms have been proposed that try to optimize the provisioning of resources based on the posed workload. These mechanisms are configured and managed by the cloud provider, imposing non negligible administration overhead. A set of challenges are identified for introducing automation and optimizing the provisioning of resources, while in parallel respecting the agreed Service Level Agreement between cloud and application providers. To address these challenges, we have developed autoscaling mechanisms for serverless applications that are powered by Reinforcement Learning (RL) techniques. A set of RL environments and agents have been implemented (based on Q-learning, DynaQ+ and Deep Q-learning algorithms) for driving autoscaling mechanisms, able to autonomously manage dynamic workloads with Quality of Service (QoS) guarantees, while opting for efficient usage of resources. The produced environments and agents are evaluated in real and simulated environments, taking advantage of the Kubeless open-source serverless platform. The evaluation results validate the suitability of the proposed mechanisms to efficiently tackle scalability management for serverless applications. © 2022 Elsevier B.V.;Zafeiropoulos A., Fotopoulou E., Filinis N., Papavassiliou S.;2022
;INFless: A native serverless system for low-latency, high-Throughput inference;"Modern websites increasingly rely on machine learning (ML) to improve their business efficiency. Developing and maintaining ML services incurs high costs for developers. Although serverless systems are a promising solution to reduce costs, we find that the current general purpose serverless systems cannot meet the low latency, high throughput demands of ML services. While simply ""patching""general serverless systems does not resolve the problem completely, we propose that such a system should natively combine the features of inference with a serverless paradigm. We present INFless, the first ML domain-specific serverless platform. It provides a unified, heterogeneous resource abstraction between CPU and accelerators, and achieves high throughput using built-in batching and non-uniform scaling mechanisms. It also supports low latency through coordinated management of batch queuing time, execution time and coldstart rate. We evaluate INFless using both a local cluster testbed and a large-scale simulation. Experimental results show that INFless outperforms state-of-The-Art systems by 2◊-5◊ on system throughput, meeting the latency goals of ML services. © 2022 ACM.";Yang Y., Zhao L., Li Y., Zhang H., Li J., Zhao M., Chen X., Li K.;2022
;Astrea: Auto-Serverless Analytics towards Cost-Efficiency and QoS-Awareness;With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of {\em serverless analytics} encounter the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of {\em Astrea}, which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. {\em Astrea} relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors ({\em e.g.}, function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain optimal job execution. We deploy {\em Astrea} in the AWS Lambda platform and conduct real-world experiments over representative benchmarks, including big data analytics and machine learning workloads, at different scales. Extensive results demonstrate that {\em Astrea} can achieve the optimal execution decision for serverless data analytics, in comparison with various provisioning and deployment baselines. For example, when compared with three provisioning baselines, {\em Astrea} manages to improve the job completion time performance by <formula><tex>$21\%$</tex></formula> to <formula><tex>$69\%$</tex></formula> under a given budget constraint, while saving cost by <formula><tex>$20\%$</tex></formula> to <formula><tex>$84\%$</tex></formula> without violating performance requirements. IEEE;Jarachanthan J., Chen L., Xu F., Li B.;2022
;Distributed Task Scheduling in Serverless Edge Computing Networks for the Internet of Things: A Learning Approach;By delegating the infrastructure management such as provisioning or scaling to third-party providers, serverless edge computing has recently been widely adopted in several applications, especially Internet of things (IoT) applications. Task scheduling is a critical issue in serverless edge computing as it significantly impacts the quality of user experience. In contrast to the centralized scheduling in the cloud center, serverless edge task scheduling is more challenging due to the heterogeneous and resource-constrained nature of edge resources. This paper aims to study the distributed task scheduling for the IoT in serverless edge computing networks, in which heterogeneous serverless edge computing nodes are rational individuals with interests to optimize their own scheduling utility while the nodes only have access to local observations. The task scheduling competition process is formulated as a partially observable stochastic game (POSG) to enable serverless edge computing nodes to non-cooperatively schedule tasks and allocate computing resources depending on their locally observed system state, which takes into account the associated task generation state, data queue state, communication channel state and previous computing resource allocation state. To solve the proposed POSG and deal with the partial observability, a multi-agent task scheduling algorithm based on the dueling double deep recurrent Q-network (D3RQN) method is developed to approximate the optimal task scheduling and resource allocation solution. Finally, extensive simulation experiments are conducted to validate the effectiveness and superiority of the proposed scheme. IEEE;Tang Q., Xie R., Yu F.R., Chen T., Zhang R., Huang T., Liu Y.;2022
